<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>transformer模型详解及代码实现 | fei's Blog</title><meta name="author" content="微冷"><meta name="copyright" content="微冷"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Transformer 输入 Transformer 中单词的输入表示 x 由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到，通常定义为 TransformerEmbedding 层，其代码实现如下所示: 单词 Embedding 单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到">
<meta property="og:type" content="article">
<meta property="og:title" content="transformer模型详解及代码实现">
<meta property="og:url" content="https://afei786.github.io/2024/02/01/transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/index.html">
<meta property="og:site_name" content="fei&#39;s Blog">
<meta property="og:description" content="Transformer 输入 Transformer 中单词的输入表示 x 由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到，通常定义为 TransformerEmbedding 层，其代码实现如下所示: 单词 Embedding 单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/12/17/J3bTOk4UgtAyHam.jpg">
<meta property="article:published_time" content="2024-02-01T05:38:35.000Z">
<meta property="article:modified_time" content="2024-02-02T08:15:25.249Z">
<meta property="article:author" content="微冷">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/12/17/J3bTOk4UgtAyHam.jpg"><link rel="shortcut icon" href="https://pic.imgdb.cn/item/659f761b871b83018a34767d.png"><link rel="canonical" href="https://afei786.github.io/2024/02/01/transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/pwa/manifest.json"/><link rel="apple-touch-icon" sizes="180x180" href="/pwa/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/pwa/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/pwa/16.png"/><link rel="mask-icon" href="/pwa/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'mediumZoom',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-left"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'transformer模型详解及代码实现',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-02 16:15:25'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (true) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic.imgdb.cn/item/659f7607871b83018a343643.jpg" onerror="onerror=null;src='https://pic.imgdb.cn/item/659f76fb871b83018a3742dc.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/12/17/J3bTOk4UgtAyHam.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="fei's Blog"><span class="site-name">fei's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">transformer模型详解及代码实现</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-01T05:38:35.000Z" title="发表于 2024-02-01 13:38:35">2024-02-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-02T08:15:25.249Z" title="更新于 2024-02-02 16:15:25">2024-02-02</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="transformer模型详解及代码实现"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>Transformer 输入</h1>
<p>Transformer 中单词的输入表示 x 由单词 Embedding 和位置 Embedding （Positional Encoding）相加得到，通常定义为 TransformerEmbedding 层，其代码实现如下所示:</p>
<h2 id="单词-Embedding">单词 Embedding</h2>
<p>单词的 Embedding 有很多种方式可以获取，例如可以采用 Word2Vec、Glove 等算法预训练得到，也可以在 Transformer 中训练得到。</p>
<h2 id="位置-Embedding">位置 Embedding</h2>
<p>Transformer 中除了单词的 Embedding，还需要使用位置 Embedding 表示单词出现在句子中的位置。因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。所以 Transformer 中使用位置 Embedding 保存单词在序列中的相对或绝对位置。</p>
<p>位置 Embedding 用 PE 表示，PE 的维度与单词 Embedding 是一样的。PE 可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>J</mi><mi>α</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">∞</mi></munderover><mfrac><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><msup><mo stretchy="false">)</mo><mi>m</mi></msup></mrow><mrow><mi>m</mi><mo stretchy="false">!</mo><mi mathvariant="normal">Γ</mi><mo stretchy="false">(</mo><mi>m</mi><mo>+</mo><mi>α</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac><msup><mrow><mo fence="true">(</mo><mfrac><mi>x</mi><mn>2</mn></mfrac><mo fence="true">)</mo></mrow><mrow><mn>2</mn><mi>m</mi><mo>+</mo><mi>α</mi></mrow></msup><mtext>，独立公式示例</mtext></mrow><annotation encoding="application/x-tex">J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0962em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.9185em;vertical-align:-1.2671em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">∞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="mclose">!</span><span class="mord">Γ</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord"><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">2</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size2">)</span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.354em;"><span style="top:-3.6029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">m</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span><span class="mord text"><span class="mord cjk_fallback">，独立公式示例</span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">PE 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">PE 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">PE</span></span></span></span></span></p>
<p>其中，pos 表示单词在句子中的位置，d 表示 PE的维度 (与词 Embedding 一样)，2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)。</p>
<h2 id="TransformerEmbedding-层实现">TransformerEmbedding 层实现</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    compute sinusoid encoding.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len, device</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        constructor of sinusoid encoding class</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param d_model: dimension of model</span></span><br><span class="line"><span class="string">        :param max_len: max sequence length</span></span><br><span class="line"><span class="string">        :param device: hardware device setting</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># same size with input matrix (for adding with input matrix)</span></span><br><span class="line">        self.encoding = torch.zeros(max_len, d_model, device=device)</span><br><span class="line">        self.encoding.requires_grad = <span class="literal">False</span>  <span class="comment"># we don&#x27;t need to compute gradient</span></span><br><span class="line"></span><br><span class="line">        pos = torch.arange(<span class="number">0</span>, max_len, device=device)</span><br><span class="line">        pos = pos.<span class="built_in">float</span>().unsqueeze(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1D =&gt; 2D unsqueeze to represent word&#x27;s position</span></span><br><span class="line"></span><br><span class="line">        _2i = torch.arange(<span class="number">0</span>, d_model, step=<span class="number">2</span>, device=device).<span class="built_in">float</span>()</span><br><span class="line">        <span class="comment"># &#x27;i&#x27; means index of d_model (e.g. embedding size = 50, &#x27;i&#x27; = [0,50])</span></span><br><span class="line">        <span class="comment"># &quot;step=2&quot; means &#x27;i&#x27; multiplied with two (same with 2 * i)</span></span><br><span class="line"></span><br><span class="line">        self.encoding[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(pos / (<span class="number">10000</span> ** (_2i / d_model)))</span><br><span class="line">        self.encoding[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(pos / (<span class="number">10000</span> ** (_2i / d_model)))</span><br><span class="line">        <span class="comment"># compute positional encoding to consider positional information of words</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># self.encoding</span></span><br><span class="line">        <span class="comment"># [max_len = 512, d_model = 512]</span></span><br><span class="line"></span><br><span class="line">        batch_size, seq_len = x.size()</span><br><span class="line">        <span class="comment"># [batch_size = 128, seq_len = 30]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.encoding[:seq_len, :]</span><br><span class="line">        <span class="comment"># [seq_len = 30, d_model = 512]</span></span><br><span class="line">        <span class="comment"># it will add with tok_emb : [128, 30, 512]         </span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TokenEmbedding</span>(nn.Embedding):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Token Embedding using torch.nn</span></span><br><span class="line"><span class="string">    they will dense representation of word using weighted matrix</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, d_model</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        class for token embedding that included positional information</span></span><br><span class="line"><span class="string">        :param vocab_size: size of vocabulary</span></span><br><span class="line"><span class="string">        :param d_model: dimensions of model</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEmbedding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    token embedding + positional encoding (sinusoid)</span></span><br><span class="line"><span class="string">    positional encoding can give positional information to network</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, max_len, d_model, drop_prob, device</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        class for word embedding that included positional information</span></span><br><span class="line"><span class="string">        :param vocab_size: size of vocabulary</span></span><br><span class="line"><span class="string">        :param d_model: dimensions of model</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(TransformerEmbedding, self).__init__()</span><br><span class="line">        self.tok_emb = TokenEmbedding(vocab_size, d_model)</span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model, max_len, device)</span><br><span class="line">        self.drop_out = nn.Dropout(p=drop_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        tok_emb = self.tok_emb(x)</span><br><span class="line">        pos_emb = self.pos_emb(x)</span><br><span class="line">        <span class="keyword">return</span> self.drop_out(tok_emb + pos_emb)</span><br></pre></td></tr></table></figure>
<h1>Transformer 整体结构</h1>
<p>论文中给出用于中英文翻译任务的 Transformer 整体架构如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/65bb31af871b83018ab43a51.jpg" alt="transformer_architecture"></p>
<p>可以看出 Transformer 架构由 Encoder 和 Decoder 两个部分组成：其中 Encoder 和 Decoder 都是由 N=6 个相同的层堆叠而成。Multi-Head Attention 结构是 Transformer 架构的核心结构，其由多个 Self-Attention 组成的。</p>
<p>Transformer 架构更详细的可视化图如下所示:<br>
<img src="https://pic.imgdb.cn/item/65bb32da871b83018ab86c7e.jpg" alt="transformer_encoder_decoder_stack"></p>
<p><img src="https://pic.imgdb.cn/item/65bb3319871b83018ab94b15.jpg" alt="transformer_resideual_layer_norm"></p>
<h2 id="2-1-Transformer-发展史">2.1 Transformer 发展史</h2>
<p>以下是 Transformer 模型（简短）历史中的一些关键节点：<br>
<img src="https://pic.imgdb.cn/item/65bb3385871b83018abadfd8.jpg" alt=""></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Transformer 架构</a> 于 2017 年 6 月推出。原本研究的重点是翻译任务。随后推出了几个有影响力的模型，包括</p>
<ul>
<li>2018 年 6 月: <a target="_blank" rel="noopener" href="https:////cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, 第一个预训练的 Transformer 模型，用于各种 NLP 任务并获得极好的结果</li>
<li>2018 年 10 月: <a target="_blank" rel="noopener" href="https:////arxiv.org/abs/1810.04805">BERT</a>, 另一个大型预训练模型，该模型旨在生成更好的句子摘要（下一章将详细介绍！）</li>
<li>2019 年 2 月: <a target="_blank" rel="noopener" href="https:////cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, GPT 的改进（并且更大）版本，由于道德问题没有立即公开发布</li>
<li>2019 年 10 月: <a target="_blank" rel="noopener" href="https:////arxiv.org/abs/1910.01108">DistilBERT</a>, BERT 的提炼版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能</li>
<li>2019 年 10 月: <a target="_blank" rel="noopener" href="https:////arxiv.org/abs/1910.13461">BART</a> 和 <a target="_blank" rel="noopener" href="https:////arxiv.org/abs/1910.10683">T5</a>, 两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）</li>
<li>2020 年 5 月: <a target="_blank" rel="noopener" href="https:////arxiv.org/abs/2005.14165">GPT-3</a>, GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习）</li>
</ul>
<p>这个列表并不全面，只是为了突出一些不同类型的 Transformer 模型。大体上，它们可以分为三类：</p>
<ul>
<li>GPT-like (也被称作自回归 Transformer 模型)</li>
<li>BERT-like (也被称作自动编码 Transformer 模型)</li>
<li>BART/T5-like (也被称作序列到序列的 Transformer 模型)</li>
</ul>
<p>Transformer 是大模型，除了一些特例（如 DistilBERT）外，实现更好性能的一般策略是增加模型的大小以及预训练的数据量。其中，GPT-2 是使用「transformer 解码器模块」构建的，而 BERT 则是通过「transformer 编码器」模块构建的。<br>
<img src="https://pic.imgdb.cn/item/65bb3569871b83018ac14d79.jpg" alt=""></p>
<table>
<thead>
<tr>
<th>模型</th>
<th>发布时间</th>
<th>参数量</th>
<th>预训练数据量</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT</td>
<td>2018 年 6 月</td>
<td>1.17 亿</td>
<td>5GB</td>
</tr>
<tr>
<td>GPT-2</td>
<td>2019 年 2 月</td>
<td>15 亿</td>
<td>40GB</td>
</tr>
<tr>
<td>GPT-3</td>
<td>2020 年 5 月</td>
<td>1,750 亿</td>
<td>45TB</td>
</tr>
</tbody>
</table>
<h1>Multi-Head Attention 结构</h1>
<p>Encoder 和 Decoder 结构中公共的 layer 之一是 Multi-Head Attention，其是由多个 Self-Attention 并行组成的。Encoder block 只包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。</p>
<p><img src="https://pic.imgdb.cn/item/65bb36b4871b83018ac54b44.jpg" alt=""></p>
<h2 id="Self-Attention-结构">Self-Attention 结构</h2>
<p>Self-Attention 中文翻译为自注意力机制，论文中叫作 Scale Dot Product Attention，它是 Transformer 架构的核心，其结构如下图所示：</p>
<p><img src="https://pic.imgdb.cn/item/65bb5437871b83018a23f4f0.jpg" alt=""></p>
<p>那么重点来了，</p>
<p>第一个问题：Self-Attention 结构的最初输入 Q(查询), K(键值), V(值) 这三个矩阵怎么理解呢？其代表什么，通过什么计算而来？</p>
<p>在 Self-Attention 中，Q、K、V 是在同一个输入（比如序列中的一个单词）上计算得到的三个向量。具体来说，我们可以通过对原始输入词的 embedding 进行线性变换（比如使用一个全连接层），来得到 Q、K、V。这三个向量的维度通常都是一样的，取决于模型设计时的决策。</p>
<p>第二个问题：Self-Attention 结构怎么理解，Q、K、V的作用是什么？这三个矩阵又怎么计算得到最后的输出？</p>
<p>在计算 Self-Attention 时，Q、K、V 被用来计算注意力分数，即用于表示当前位置和其他位置之间的关系。注意力分数可以通过 Q 和 K 的点积来计算，然后将分数除以 8，再经过一个 softmax 归一化处理，得到每个位置的权重。然后用这些权重来加权计算 V 的加权和，即得到当前位置的输出。</p>
<p>将分数除以 8 的操作，对应图中的 Scale 层，这个参数 8 是 K 向量维度 64 的平方根结果。</p>
<h2 id="Self-Attention-实现">Self-Attention 实现</h2>
<p>文章的 Self-Attention 层和论文中的 ScaleDotProductAttention 层意义是一样的。<br>
输入序列单词的 Embedding Vector 经过线性变换（Linear 层）得到 Q、K、V 三个向量，并将它们作为 Self-Attention 层的输入。假设输入序列的长度为 seq_len，则 Q、K 和 V 的形状为（seq_len，d_k），其中，<br>
表示每个词或向量的维度，也是<br>
、<br>
矩阵的列数。在论文中，输入给 Self-Attention 层的 Q、K、V 的向量维度是 64， Embedding Vector 和 Encoder-Decoder 模块输入输出的维度都是 512。</p>
<p>Embedding Vector 的大小是我们可以设置的超参数—基本上它就是我们训练数据集中最长句子的长度。<br>
Self-Attention 层的计算过程用数学公式可表达为:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4483em;vertical-align:-0.93em;"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5183em;"><span style="top:-2.2528em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8572em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8172em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1828em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span></span></p>
<p>以下是一个示例代码，它创建了一个 ScaleDotProductAttention 层，并将 Q、K、V 三个张量传递给它进行计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaleDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaleDotProductAttention, self).__init__()</span><br><span class="line">        self.softmax = nn.Softmax(dim = -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, mask=<span class="literal">None</span></span>):</span><br><span class="line">        K_T = K.transpose(-<span class="number">1</span>, -<span class="number">2</span>) <span class="comment"># 计算矩阵 K 的转置  </span></span><br><span class="line">        d_k = Q.size(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 1, 计算 Q, K^T 矩阵的点积，再除以 sqrt(d_k) 得到注意力分数矩阵</span></span><br><span class="line">        scores = torch.matmul(Q, K_T) / math.sqrt(d_k)</span><br><span class="line">        <span class="comment"># 2, 如果有掩码，则将注意力分数矩阵中对应掩码位置的值设为负无穷大</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        <span class="comment"># 3, 对注意力分数矩阵按照最后一个维度进行 softmax 操作，得到注意力权重矩阵，值范围为 [0, 1]</span></span><br><span class="line">        attn_weights = self.softmax(scores)</span><br><span class="line">        <span class="comment"># 4, 将注意力权重矩阵乘以 V，得到最终的输出矩阵</span></span><br><span class="line">        output = torch.matmul(attn_weights, V)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, attn_weights</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Q、K、V 三个张量</span></span><br><span class="line">Q = torch.randn(<span class="number">5</span>, <span class="number">10</span>, <span class="number">64</span>)  <span class="comment"># (batch_size, sequence_length, d_k)</span></span><br><span class="line">K = torch.randn(<span class="number">5</span>, <span class="number">10</span>, <span class="number">64</span>)  <span class="comment"># (batch_size, sequence_length, d_k)</span></span><br><span class="line">V = torch.randn(<span class="number">5</span>, <span class="number">10</span>, <span class="number">64</span>)  <span class="comment"># (batch_size, sequence_length, d_k)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 ScaleDotProductAttention 层</span></span><br><span class="line">attention = ScaleDotProductAttention()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 Q、K、V 三个张量传递给 ScaleDotProductAttention 层进行计算</span></span><br><span class="line">output, attn_weights = attention(Q, K, V)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印输出矩阵和注意力权重矩阵的形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ScaleDotProductAttention output shape: <span class="subst">&#123;output.shape&#125;</span>&quot;</span>) <span class="comment"># torch.Size([5, 10, 64])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;attn_weights shape: <span class="subst">&#123;attn_weights.shape&#125;</span>&quot;</span>) <span class="comment"># torch.Size([5, 10, 10])</span></span><br></pre></td></tr></table></figure>
<h2 id="Multi-Head-Attention">Multi-Head Attention</h2>
<p>Multi-Head Attention (MHA) 是基于Self-Attention (SA) 的一种变体。MHA 在 SA 的基础上引入了“多头”机制，将输入拆分为多个子空间，每个子空间分别执行 SA，最后将多个子空间的输出拼接在一起并进行线性变换，从而得到最终的输出。</p>
<p>对于 MHA，之所以需要对 Q、K、V 进行多头（head）划分，其目的是为了增强模型对不同信息的关注。具体来说，多组 Q、K、V 分别计算 Self-Attention，每个头自然就会有独立的 Q、K、V 参数，从而让模型同时关注多个不同的信息，这有些类似 CNN 架构模型的多通道机制。</p>
<p>下图是论文中 Multi-Head Attention 的结构图。<br>
<img src="https://pic.imgdb.cn/item/65bb55b0871b83018a288535.jpg" alt=""></p>
<p>从图中可以看出， MHA 结构的计算过程可总结为下述步骤:</p>
<ul>
<li>将输入 Q、K、V 张量进行线性变换（Linear 层），输出张量尺寸为 [batch_size, seq_len, d_model]；</li>
<li>将前面步骤输出的张量，按照头的数量（n_head）拆分为 n_head 子张量，其尺寸为 [batch_size, n_head, seq_len, d_model//n_head]；</li>
<li>每个子张量并行计算注意力分数，即执行 dot-product attention 层，输出张量尺寸为 [batch_size, n_head, seq_len, d_model//n_head]；</li>
<li>将这些子张量进行拼接 concat ，并经过线性变换得到最终的输出张量，尺寸为 [batch_size, seq_len, d_model]。</li>
</ul>
<p>总结：因为 GPU 的并行计算特性，步骤2中的张量拆分和步骤4中的张量拼接，其实都是通过 review 算子来实现的。同时，也能发现SA 和 MHA 模块的输入输出矩阵维度都是一样的。</p>
<h2 id="Multi-Head-Attention-实现">Multi-Head Attention 实现</h2>
<p>Multi-Head Attention 层的输入同样也是三个张量：查询（Query）、键（Key）和值（Value），其计算过程用数学公式可表达为:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>M</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>d</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>C</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy="false">(</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">Multihead(Q,K,V)=Concat(head_1,...,head_h)W^O
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">u</span><span class="mord mathnormal">lt</span><span class="mord mathnormal">ih</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1413em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">O</span></span></span></span></span></span></span></span></span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>I</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">head_i=Attention(QW^Q_i,KW^K_i,VW^V_I)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2361em;vertical-align:-0.2769em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9592em;"><span style="top:-2.4231em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.1809em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.453em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07847em;">I</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.22222em;">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>一般用 d_model 表示输入嵌入向量的维度， n_head 表示分割成多少个头，因此，d_model//n_head 自然表示每个头的输入和输出维度，在论文中 d_model = 512，n_head = 8，d_model//n_head = 64。值得注意的是，由于每个头的维数减少，总计算成本与具有全维的单头注意力是相似的。</p>
<p>Multi-Head Attention 层的 Pytorch 实现代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multi-Head Attention Layer</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        d_model: Dimensions of the input embedding vector, equal to input and output dimensions of each head</span></span><br><span class="line"><span class="string">        n_head: number of heads, which is also the number of parallel attention layers</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.attention = ScaleDotProductAttention()</span><br><span class="line">        self.w_q = nn.Linear(d_model, d_model)  <span class="comment"># Q 线性变换层</span></span><br><span class="line">        self.w_k = nn.Linear(d_model, d_model)  <span class="comment"># K 线性变换层</span></span><br><span class="line">        self.w_v = nn.Linear(d_model, d_model)  <span class="comment"># V 线性变换层</span></span><br><span class="line">        self.fc = nn.Linear(d_model, d_model)   <span class="comment"># 输出线性变换层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 1. dot product with weight matrices</span></span><br><span class="line">        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v) <span class="comment"># size is [batch_size, seq_len, d_model]</span></span><br><span class="line">        <span class="comment"># 2, split by number of heads(n_head) # size is [batch_size, n_head, seq_len, d_model//n_head]</span></span><br><span class="line">        q, k, v = self.split(q), self.split(k), self.split(v)</span><br><span class="line">        <span class="comment"># 3, compute attention</span></span><br><span class="line">        sa_output, attn_weights = self.attention(q, k, v, mask)</span><br><span class="line">        <span class="comment"># 4, concat attention and linear transformation</span></span><br><span class="line">        concat_tensor = self.concat(sa_output)</span><br><span class="line">        mha_output = self.fc(concat_tensor)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> mha_output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split</span>(<span class="params">self, tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        split tensor by number of head(n_head)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tensor: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="string">        :return: [batch_size, n_head, seq_len, d_model//n_head], 输出矩阵是四维的，第二个维度是 head 维度</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 将 Q、K、V 通过 reshape 函数拆分为 n_head 个头</span></span><br><span class="line"><span class="string">        batch_size, seq_len, _ = q.shape</span></span><br><span class="line"><span class="string">        q = q.reshape(batch_size, seq_len, n_head, d_model // n_head)</span></span><br><span class="line"><span class="string">        k = k.reshape(batch_size, seq_len, n_head, d_model // n_head)</span></span><br><span class="line"><span class="string">        v = v.reshape(batch_size, seq_len, n_head, d_model // n_head)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        batch_size, seq_len, d_model = tensor.size()</span><br><span class="line">        d_tensor = d_model // self.n_head</span><br><span class="line">        split_tensor = tensor.view(batch_size, seq_len, self.n_head, d_tensor).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># it is similar with group convolution (split by number of heads)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> split_tensor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">concat</span>(<span class="params">self, sa_output</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; merge multiple heads back together</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            sa_output: [batch_size, n_head, seq_len, d_tensor]</span></span><br><span class="line"><span class="string">            return: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, n_head, seq_len, d_tensor = sa_output.size()</span><br><span class="line">        d_model = n_head * d_tensor</span><br><span class="line">        concat_tensor = sa_output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, seq_len, d_model)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> concat_tensor</span><br></pre></td></tr></table></figure>
<h1>Encoder 结构</h1>
<p>Encoder 结构由N=6个相同的 encoder block 堆叠而成，每一层（ layer）主要有两个子层（sub-layers）: 第一个子层是多头注意力机制（Multi-Head Attention），第二个是简单的位置全连接前馈网络（Positionwise Feed Forward）。<br>
<img src="https://pic.imgdb.cn/item/65bb630e871b83018a51f80d.jpg" alt=""></p>
<p>上图红色框框出的部分是 Encoder block，很明显其是 Multi-Head Attention、Add&amp;Norm、Feed Forward、Add &amp; Norm 层组成的。另外在论文中 Encoder 组件由 N=6个相同的 encoder block 堆叠而成，且 encoder block 输入矩阵和输出矩阵维度是一样的。</p>
<h2 id="Add-Norm">Add &amp; Norm</h2>
<p>Add &amp; Norm 层由 Add 和 Norm 两部分组成。这里的 Add 指 X + MultiHeadAttention(X)，是一种残差连接。Norm 是 Layer Normalization。Add &amp; Norm 层计算过程用数学公式可表达为:</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>X</mi><mo>+</mo><mi>M</mi><mi>u</mi><mi>l</mi><mi>t</mi><mi>i</mi><mi>H</mi><mi>e</mi><mi>a</mi><mi>d</mi><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">LayerNorm(X+MultiHeadAttention(X))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em;">yer</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.02778em;">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">u</span><span class="mord mathnormal">lt</span><span class="mord mathnormal">i</span><span class="mord mathnormal">He</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">))</span></span></span></span></span></p>
<p>Add 比较简单，这里重点讲下 Layer Norm 层。Layer Norm 是一种常用的神经网络归一化技术，可以使得模型训练更加稳定，收敛更快。它的主要作用是对每个样本在特征维度上进行归一化，减少了不同特征之间的依赖关系，提高了模型的泛化能力。Layer Norm 层的计算可视化如下图所示:<br>
<img src="https://pic.imgdb.cn/item/65bb6413871b83018a54f647.jpg" alt=""><br>
Layer Norm 层的 Pytorch 实现代码如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, eps=<span class="number">1e-12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.gamma = nn.Parameter(torch.ones(d_model))</span><br><span class="line">        self.beta = nn.Parameter(torch.zeros(d_model))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># &#x27;-1&#x27; means last dimension. </span></span><br><span class="line">        var = x.var(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        out = (x - mean) / torch.sqrt(var + self.eps)</span><br><span class="line">        out = self.gamma * out + self.beta</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># NLP Example</span></span><br><span class="line">batch, sentence_length, embedding_dim = <span class="number">20</span>, <span class="number">5</span>, <span class="number">10</span></span><br><span class="line">embedding = torch.randn(batch, sentence_length, embedding_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1，Activate nn.LayerNorm module</span></span><br><span class="line">layer_norm1 = nn.LayerNorm(embedding_dim)</span><br><span class="line">pytorch_ln_out = layer_norm1(embedding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2，Activate my nn.LayerNorm module</span></span><br><span class="line">layer_norm2 = LayerNorm(embedding_dim)</span><br><span class="line">my_ln_out = layer_norm2(embedding)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较结果</span></span><br><span class="line"><span class="built_in">print</span>(torch.allclose(pytorch_ln_out, my_ln_out, rtol=<span class="number">0.1</span>,atol=<span class="number">0.01</span>))  <span class="comment"># 输出 True</span></span><br></pre></td></tr></table></figure>
<h2 id="Feed-Forward">Feed Forward</h2>
<p><img src="https://pic.imgdb.cn/item/65bb6459871b83018a55c2cd.jpg" alt=""></p>
<p>Feed Forward 层全称是 Position-wise Feed-Forward Networks，其本质是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数，计算过程用数学公式可表达为：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>X</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">FFN(X)=max(0,XW_1+b_1)W_2+b_2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FFN</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>除了使用两个全连接层来完成线性变换，另外一种方式是使用 kernal_size = 1 的两个1X1 卷积层，输入输出维度不变，都是 512，中间维度是 2048。</p>
<p>PositionwiseFeedForward 层的 Pytorch 实现代码如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_diff, drop_prob=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(d_model, d_diff)</span><br><span class="line">        self.fc2 = nn.Linear(d_diff, d_model)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.dropout = nn.Dropout(drop_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="Encoder-结构的实现">Encoder 结构的实现</h2>
<p>基于前面 Multi-Head Attention, Feed Forward, Add &amp; Norm 的内容我们可以完整的实现 Encoder 结构。<br>
<img src="https://pic.imgdb.cn/item/65bb65c5871b83018a59c9ac.jpg" alt=""></p>
<p>Encoder 组件的 Pytorch 实现代码如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, ffn_hidden, n_head, drop_prob=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.mha = MultiHeadAttention(d_model, n_head)</span><br><span class="line">        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden)</span><br><span class="line">        self.ln1 = LayerNorm(d_model)</span><br><span class="line">        self.ln2 = LayerNorm(d_model)</span><br><span class="line">        self.dropout1 = nn.Dropout(drop_prob)</span><br><span class="line">        self.dropout2 = nn.Dropout(drop_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line">        x_residual1 = x</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1, compute multi-head attention</span></span><br><span class="line">        x = self.mha(q=x, k=x, v=x, mask=mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2, add residual connection and apply layer norm</span></span><br><span class="line">        x = self.ln1( x_residual1 + self.dropout1(x) )</span><br><span class="line">        x_residual2 = x</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3, compute position-wise feed forward</span></span><br><span class="line">        x = self.ffn(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4, add residual connection and apply layer norm</span></span><br><span class="line">        x = self.ln2( x_residual2 + self.dropout2(x) )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_voc_size, seq_len, d_model, ffn_hidden, n_head, n_layers, drop_prob=<span class="number">0.1</span>, device=<span class="string">&#x27;cpu&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.emb = TransformerEmbedding(vocab_size = enc_voc_size,</span><br><span class="line">                                        max_len = seq_len,</span><br><span class="line">                                        d_model = d_model,</span><br><span class="line">                                        drop_prob = drop_prob,</span><br><span class="line">                                        device=device)</span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer(d_model, ffn_hidden, n_head, drop_prob) </span><br><span class="line">                                     <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        x = self.emb(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h1>Decoder 结构</h1>
<p><img src="https://pic.imgdb.cn/item/65bb661f871b83018a5ac2bf.jpg" alt=""></p>
<p>上图右边红框框出来的是 Decoder block，Decoder 组件也是由N=6个相同的 Decoder block 堆叠而成。Decoder block 与 Encoder block 相似，但是存在一些区别：</p>
<ul>
<li>包含两个 Multi-Head Attention 层。</li>
<li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li>
<li>第二个 Multi-Head Attention 层的 K, V 矩阵使用 Encoder 的编码信息矩阵 C 进行计算，而 Q 使用上一个 Decoder block 的输出计算。这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)</li>
</ul>
<p>注意，解码器块中的第一个注意力层关联到解码器的所有（过去的）输入，但是第二注意力层使用编码器的输出。因此，它可以访问整个输入句子，以最好地预测当前单词。这是非常有用的，因为不同的语言可以有语法规则将单词按不同的顺序排列，或者句子后面提供的一些上下文可能有助于确定给定单词的最佳翻译。</p>
<p>另外，Decoder 组件后面还会接一个全连接层和 Softmax 层计算下一个翻译单词的概率。</p>
<p>Decoder 组件的代码实现如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, ffn_hidden, n_head, drop_prob</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.mha1 = MultiHeadAttention(d_model, n_head)</span><br><span class="line">        self.ln1 = LayerNorm(d_model)</span><br><span class="line">        self.dropout1 = nn.Dropout(p=drop_prob)</span><br><span class="line"></span><br><span class="line">        self.mha2 = MultiHeadAttention(d_model, n_head)</span><br><span class="line">        self.ln2 = LayerNorm(d_model)</span><br><span class="line">        self.dropout2 = nn.Dropout(p=drop_prob)</span><br><span class="line"></span><br><span class="line">        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden)</span><br><span class="line">        self.ln3 = LayerNorm(d_model)</span><br><span class="line">        self.dropout3 = nn.Dropout(p=drop_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_out, enc_out, trg_mask, src_mask</span>):</span><br><span class="line">        x_residual1 = dec_out</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1, compute multi-head attention</span></span><br><span class="line">        x = self.mha1(q=dec_out, k=dec_out, v=dec_out, mask=trg_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2, add residual connection and apply layer norm</span></span><br><span class="line">        x = self.ln1( x_residual1 + self.dropout1(x) )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> enc_out <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 3, compute encoder - decoder attention</span></span><br><span class="line">            x_residual2 = x</span><br><span class="line">            x = self.mha2(q=x, k=enc_out, v=enc_out, mask=src_mask)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 4, add residual connection and apply layer norm</span></span><br><span class="line">            x = self.ln2( x_residual2 + self.dropout2(x) )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. positionwise feed forward network</span></span><br><span class="line">        x_residual3 = x</span><br><span class="line">        x = self.ffn(x)</span><br><span class="line">        <span class="comment"># 6, add residual connection and apply layer norm</span></span><br><span class="line">        x = self.ln3( x_residual3 + self.dropout3(x) )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.emb = TransformerEmbedding(d_model=d_model,</span><br><span class="line">                                        drop_prob=drop_prob,</span><br><span class="line">                                        max_len=max_len,</span><br><span class="line">                                        vocab_size=dec_voc_size,</span><br><span class="line">                                        device=device)</span><br><span class="line"></span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,</span><br><span class="line">                                                  ffn_hidden=ffn_hidden,</span><br><span class="line">                                                  n_head=n_head,</span><br><span class="line">                                                  drop_prob=drop_prob)</span><br><span class="line">                                     <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(d_model, dec_voc_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, trg, src, trg_mask, src_mask</span>):</span><br><span class="line">        trg = self.emb(trg)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            trg = layer(trg, src, trg_mask, src_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass to LM head</span></span><br><span class="line">        output = self.linear(trg)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h1>Transformer 总结</h1>
<ul>
<li>Transformer 与 RNN 不同，可以比较好地并行训练。</li>
<li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li>
<li>Transformer 的重点是 Self-Attention 结构，其中用到的 Q, K, V矩阵通过输出进行线性变换得到。</li>
<li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score。</li>
</ul>
<h2 id="Transformer-完整代码实现">Transformer 完整代码实现</h2>
<p>基于前面实现的 Encoder 和 Decoder 组件，我们可以实现 Transformer 模型的完整代码，如下所示:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> models.model.decoder <span class="keyword">import</span> Decoder</span><br><span class="line"><span class="keyword">from</span> models.model.encoder <span class="keyword">import</span> Encoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,</span></span><br><span class="line"><span class="params">                 ffn_hidden, n_layers, drop_prob, device</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.src_pad_idx = src_pad_idx</span><br><span class="line">        self.trg_pad_idx = trg_pad_idx</span><br><span class="line">        self.trg_sos_idx = trg_sos_idx</span><br><span class="line">        self.device = device</span><br><span class="line">        self.encoder = Encoder(d_model=d_model,</span><br><span class="line">                               n_head=n_head,</span><br><span class="line">                               max_len=max_len,</span><br><span class="line">                               ffn_hidden=ffn_hidden,</span><br><span class="line">                               enc_voc_size=enc_voc_size,</span><br><span class="line">                               drop_prob=drop_prob,</span><br><span class="line">                               n_layers=n_layers,</span><br><span class="line">                               device=device)</span><br><span class="line"></span><br><span class="line">        self.decoder = Decoder(d_model=d_model,</span><br><span class="line">                               n_head=n_head,</span><br><span class="line">                               max_len=max_len,</span><br><span class="line">                               ffn_hidden=ffn_hidden,</span><br><span class="line">                               dec_voc_size=dec_voc_size,</span><br><span class="line">                               drop_prob=drop_prob,</span><br><span class="line">                               n_layers=n_layers,</span><br><span class="line">                               device=device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, trg</span>):</span><br><span class="line">        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)</span><br><span class="line"></span><br><span class="line">        src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)</span><br><span class="line"></span><br><span class="line">        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * \</span><br><span class="line">                   self.make_no_peak_mask(trg, trg)</span><br><span class="line"></span><br><span class="line">        enc_src = self.encoder(src, src_mask)</span><br><span class="line">        output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_pad_mask</span>(<span class="params">self, q, k, q_pad_idx, k_pad_idx</span>):</span><br><span class="line">        len_q, len_k = q.size(<span class="number">1</span>), k.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># batch_size x 1 x 1 x len_k</span></span><br><span class="line">        k = k.ne(k_pad_idx).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># batch_size x 1 x len_q x len_k</span></span><br><span class="line">        k = k.repeat(<span class="number">1</span>, <span class="number">1</span>, len_q, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># batch_size x 1 x len_q x 1</span></span><br><span class="line">        q = q.ne(q_pad_idx).unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># batch_size x 1 x len_q x len_k</span></span><br><span class="line">        q = q.repeat(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, len_k)</span><br><span class="line"></span><br><span class="line">        mask = k &amp; q</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_no_peak_mask</span>(<span class="params">self, q, k</span>):</span><br><span class="line">        len_q, len_k = q.size(<span class="number">1</span>), k.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># len_q x len_k</span></span><br><span class="line">        mask = torch.tril(torch.ones(len_q, len_k)).<span class="built_in">type</span>(torch.BoolTensor).to(self.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ugWDIIOHtPA&amp;list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&amp;index=61">李宏毅 Transformer</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://afei786.github.io">微冷</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://afei786.github.io/2024/02/01/transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/">https://afei786.github.io/2024/02/01/transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://afei786.github.io" target="_blank">fei's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/12/17/J3bTOk4UgtAyHam.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/02/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="模型压缩"><img class="cover" src="https://s2.loli.net/2023/12/17/J3bTOk4UgtAyHam.jpg" onerror="onerror=null;src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">模型压缩</div></div></a></div><div class="next-post pull-right"><a href="/2024/01/10/opencv-compile-install/" title="opencv_compile_install"><img class="cover" src="https://s2.loli.net/2023/12/17/HOU7WMvzXZdkuSy.jpg" onerror="onerror=null;src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">opencv_compile_install</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic.imgdb.cn/item/659f7607871b83018a343643.jpg" onerror="this.onerror=null;this.src='https://pic.imgdb.cn/item/659f76fb871b83018a3742dc.gif'" alt="avatar"/></div><div class="author-info__name">微冷</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/dashboard"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/afei786" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:chenxinsunfei@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">Transformer 输入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%95%E8%AF%8D-Embedding"><span class="toc-number">1.1.</span> <span class="toc-text">单词 Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE-Embedding"><span class="toc-number">1.2.</span> <span class="toc-text">位置 Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TransformerEmbedding-%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.</span> <span class="toc-text">TransformerEmbedding 层实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">Transformer 整体结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Transformer-%E5%8F%91%E5%B1%95%E5%8F%B2"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Transformer 发展史</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">Multi-Head Attention 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Attention-%E7%BB%93%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">Self-Attention 结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Self-Attention-%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.</span> <span class="toc-text">Self-Attention 实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Head-Attention"><span class="toc-number">3.3.</span> <span class="toc-text">Multi-Head Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Multi-Head-Attention-%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.4.</span> <span class="toc-text">Multi-Head Attention 实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">Encoder 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Add-Norm"><span class="toc-number">4.1.</span> <span class="toc-text">Add &amp; Norm</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Feed-Forward"><span class="toc-number">4.2.</span> <span class="toc-text">Feed Forward</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-%E7%BB%93%E6%9E%84%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.3.</span> <span class="toc-text">Encoder 结构的实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">Decoder 结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">Transformer 总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Transformer-%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">6.1.</span> <span class="toc-text">Transformer 完整代码实现</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/06/21/docker-vscode/" title="docker-vscode"><img src="https://s2.loli.net/2023/12/17/h6wHkYULnpNROsG.jpg" onerror="this.onerror=null;this.src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="docker-vscode"/></a><div class="content"><a class="title" href="/2024/06/21/docker-vscode/" title="docker-vscode">docker-vscode</a><time datetime="2024-06-21T14:09:53.000Z" title="发表于 2024-06-21 22:09:53">2024-06-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/11/my-information/" title="my-information"><img src="https://s2.loli.net/2023/12/17/pvLabdPRhsXr819.jpg" onerror="this.onerror=null;this.src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="my-information"/></a><div class="content"><a class="title" href="/2024/06/11/my-information/" title="my-information">my-information</a><time datetime="2024-06-11T14:16:40.000Z" title="发表于 2024-06-11 22:16:40">2024-06-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/06/10/docker-nvidia/" title="docker-nvidia"><img src="https://s2.loli.net/2023/12/17/h6wHkYULnpNROsG.jpg" onerror="this.onerror=null;this.src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="docker-nvidia"/></a><div class="content"><a class="title" href="/2024/06/10/docker-nvidia/" title="docker-nvidia">docker-nvidia</a><time datetime="2024-06-10T14:49:18.000Z" title="发表于 2024-06-10 22:49:18">2024-06-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/26/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E7%AF%87/" title="算法总结篇"><img src="https://s2.loli.net/2023/12/17/OdmekDlcEhCwSZN.jpg" onerror="this.onerror=null;this.src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="算法总结篇"/></a><div class="content"><a class="title" href="/2024/02/26/%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E7%AF%87/" title="算法总结篇">算法总结篇</a><time datetime="2024-02-26T08:57:50.000Z" title="发表于 2024-02-26 16:57:50">2024-02-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/02/21/%E9%9D%A2%E8%AF%95%E9%A2%98/" title="面试题"><img src="https://s2.loli.net/2023/12/17/OdmekDlcEhCwSZN.jpg" onerror="this.onerror=null;this.src='https://pic.imgdb.cn/item/659f7624871b83018a3495b7.jpg'" alt="面试题"/></a><div class="content"><a class="title" href="/2024/02/21/%E9%9D%A2%E8%AF%95%E9%A2%98/" title="面试题">面试题</a><time datetime="2024-02-21T10:59:48.000Z" title="发表于 2024-02-21 18:59:48">2024-02-21</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://s2.loli.net/2023/12/17/J3bTOk4UgtAyHam.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 微冷</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="false" async="async"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>